
```{r, include=FALSE}
library(knitr)
library(kableExtra)
library(forecast)
library(astsa)
```


## 3.2 Data Description and Division

```{r, fig.width=8, fig.height=5, fig.align='center', out.width='80%', echo=FALSE}
#read and plot the dataset
JNJ = read.csv("Data_Group8.csv", header = TRUE)
kable(head(JNJ, 5), caption = "First Five Rows of JNJ Historical Stock Prices")
JNJ.ts = ts(JNJ$Adj.Close, start = c(1985, 1), end = c(2024, 3), frequency = 12) #We only need Adf.Close as data
plot(JNJ.ts, ylab = "JNJ Adjusted Close Stock Prices",
     main = "Time Series Plot of JNJ Adjusted Close Stock Price
     from January 1985 to March 2024")
```


```{r, fig.width=8, fig.height=5, fig.align='center', out.width='80%', echo=FALSE}
#plot the train set and test set
JNJ.ts = window(JNJ.ts, start = c(2014, 4), end = c(2024, 3))
JNJ.train = window(JNJ.ts, start = c(2014, 4), end = c(2023, 3)) #Train set
JNJ.test = window(JNJ.ts, start = c(2023, 4), end = c(2024, 3)) #Test set
plot(JNJ.ts, ylab = "JNJ Adjusted Close Stock Prices",
     main = "Time Series Plot of JNJ Adjusted Close Stock Price
     from April 2014 to March 2024
     with Training and Testing Dataset Divisions", ylim = c(70, 180))
lines(JNJ.train, col = "seagreen")
lines(JNJ.test, col = "dodgerblue")
abline(v = 2023+3/12, lty = 2)
legend("topleft", c("Training Set", "Test Set"), col = c("seagreen", "dodgerblue"), lty = 1, bty = "n")
```


## 3.3 Possible Solutions


```{r, echo=FALSE}
# Fligner-Killeen test on the data
seg = rep(1:10, each = 12)
fligner.test(JNJ.ts, seg)
```


# 4 Analysis

## 4.1 Regression Modelling



```{r, echo=FALSE, fig.width=7, fig.height=6.5, fig.align='center', out.width="70%"}
#only consider the trend
mse.regression1=msi.regression1=rep(0,15)
X=poly(time(JNJ.ts),15)
for (p in 1:15){
  mod.regression1=lm(JNJ.train ~ X[1:108, 1:p])
  predict.regression1=cbind(1,X[-(1:108),1:p]) %*% coef(mod.regression1)
  mse.regression1[p]=mean((JNJ.test - predict.regression1)^2)
  msi.regression1[p]=mean(1-JNJ.test/predict.regression1)^2
}
par(mfrow=c(2,2))
plot(mse.regression1, pch=16, xlab = "p", ylab = "MSE",
     main = "Quadratic Loss Function vs. p \n (only considering trend)", cex.main = 0.9)
points(x = which.min(mse.regression1), y = min(mse.regression1), col = "firebrick3", pch = 19)
legend("topleft", pch = 19, bty = "n", cex = 0.9, legend = "Minimum MSE", col = "firebrick3")
plot(msi.regression1, pch=16, xlab = "p", ylab = "MSI",
     main = "Scale Invariant Loss Function vs. p \n (only considering trend)", cex.main = 0.9)
points(x = which.min(msi.regression1), y = min(msi.regression1), col = "firebrick3", pch = 19)
legend("topleft", pch = 19, bty = "n", cex = 0.9, legend = "Minimum MSI", col = "firebrick3")

#consider both the trend and seasonality
mse.regression2=msi.regression2=rep(0,15)
month=as.factor(cycle(JNJ.train))
for (p in 1:15){
  mod.regression2=lm(JNJ.train~X[1:108,1:p]+month)
  predict.regression2=model.matrix(~X[-(1:108),1:p]+as.factor(cycle(JNJ.test)))%*% coef(mod.regression2)
  mse.regression2[p]=mean((JNJ.test - predict.regression2)^2)
  msi.regression2[p]=mean(1-JNJ.test/predict.regression2)^2
}
plot(mse.regression2, pch=16, xlab = "p", ylab = "MSE",
     main = "Quadratic Loss Function vs. p \n (considering both trend and seasonality)", cex.main = 0.9)
points(x = which.min(mse.regression1), y = min(mse.regression1), col = "firebrick3", pch = 19)
legend("topleft", pch = 19, bty = "n", cex = 0.9, legend = "Minimum MSE", col = "firebrick3")
plot(msi.regression2, pch=16, xlab = "p", ylab = "MSI",
     main = "Scale Invariant Loss Function vs. p\n (considering both trend and seasonality)", cex.main = 0.9)
points(x = which.min(msi.regression1), y = min(msi.regression1), col = "firebrick3", pch = 19)
legend("topleft", pch = 19, bty = "n", cex = 0.9, legend = "Minimum MSI", col = "firebrick3")
```


```{r, echo=FALSE}
mse.msi = data.frame(Regression=c("Only consider the trend","Consider both the trend and seasonality"),
                     MSE=c(min(mse.regression1),
                           min(mse.regression2)),
                     MSI=c(min(msi.regression1),
                           min(msi.regression2)))
kable(mse.msi)
p.best=4
```


### 4.1.2 Model Diagnostics


```{r, fig.width=12, fig.height=6, fig.align='center', out.width='78%', echo=FALSE, results="hide"}
#Check if the chosen model satisfy the error term assumptions
mod.regression=lm(JNJ.train~X[1:108,1:p.best])

par(mfrow=c(2,2), cex.main=1.5)
plot(mod.regression$fitted,mod.regression$residuals,pch=16, col=adjustcolor("black",0.5),
     xlab="fitted value", ylab="residuals", main = "Residuals vs. Fitted Values")
abline(h=0,col="red",lty=2,lwd=2)

car::qqPlot(mod.regression$residuals,pch=16,col=adjustcolor("black",0.7),xlab="Theoretical Quantiles (Normal)",ylab="Sample Quantiles", main="Normal QQ Plot")

plot(mod.regression$residuals,pch=16,col=adjustcolor("black",0.5),ylab="residuals", main = "Residuals vs. Time")
abline(h=0,col="red",lty=2,lwd=2)

acf(mod.regression$residuals,main="ACF Plot of Residuals")
```
```{r, echo=FALSE}
shapiro.test(residuals(mod.regression))
segment = rep(1:9, each = 12)
fligner.test(residuals(mod.regression),segment)
randtests::difference.sign.test(residuals(mod.regression))
randtests::runs.test(residuals(mod.regression))
```


### 4.1.3 Prediction Power

```{r,echo=FALSE, fig.width=8, fig.height=6, fig.align='center', out.width="70%"}
tim=as.numeric(time(JNJ.train))
final.regression=lm(JNJ.train~poly(tim,p.best))
#predict for next 12 months after train set ends
new=data.frame(tim=seq(2023+4/12,2024+3/12,by=1/12))
prediction1=predict(final.regression,new,interval="prediction",level=0.95)
prediction2=predict(final.regression,new,interval="prediction",level=0.9)
plot(JNJ.train,xlim=c(2014,2025),ylim=c(50,200), main="Regression Prediction Power")
abline(v=2023+3/12,lty=2)

lines(JNJ.train,col="seagreen",lwd=2)
lines(JNJ.test,col="dodgerblue",lwd=2)

lines(new$tim,prediction1[,1],col="red",lty=1,lwd=2)
lines(new$tim,prediction1[,2],col="grey50",lty=2,lwd=2)
lines(new$tim,prediction1[,3],col="grey80",lty=2,lwd=2)
lines(new$tim,prediction2[,2],col="grey80",lty=2,lwd=2)
lines(new$tim,prediction2[,3],col="grey50",lty=2,lwd=2)
legend("topleft", lty = 1, bty = "n",
       c("Training Set", "Test Set", "Prediction",
         "90% Prediction Interval", "95% Prediction Interval"),
       col = c("seagreen", "dodgerblue", "red", "grey80", "grey50"))
```
```{r,include=FALSE}
reg.apse = mean((JNJ.test - predict(final.regression, newdata=new))^2); reg.apse
```


## 4.2 Smoothing Methods



### 4.2.1 Model Selection


```{r,echo=FALSE}
# simple exponential smoothing
smoother1 = HoltWinters(JNJ.train, gamma = FALSE, beta = FALSE)
# double exponential smoothing
smoother2 = HoltWinters(JNJ.train, gamma = FALSE)
# additive HW
smoother3 = HoltWinters(JNJ.train, seasonal = "additive")
# multiplicative HW
smoother4 = HoltWinters(JNJ.train, seasonal = "multiplicative")

# Calculate and list APSE's for the 4 models
HW.predict1 = predict(smoother1, n.ahead = length(JNJ.test))
APSE1 = mean((JNJ.test - HW.predict1)^2)
HW.predict2 = predict(smoother2, n.ahead = length(JNJ.test))
APSE2 = mean((JNJ.test - HW.predict2)^2)
HW.predict3 = predict(smoother3, n.ahead = length(JNJ.test))
APSE3 = mean((JNJ.test - HW.predict3)^2)
HW.predict4 = predict(smoother4, n.ahead = length(JNJ.test))
APSE4 = mean((JNJ.test - HW.predict4)^2)

HW.final <- HoltWinters(JNJ.train, gamma = FALSE)
kable(data.frame(APSE1, APSE2, APSE3, APSE4),
      col.names = c("simple exponential","double exponential","additive HW","multiplicative HW"))
smoother2
```


### 4.2.2 Model Diagnostics


```{r,echo=FALSE, fig.width=12, fig.height=3.9, fig.align='center', out.width='75%'}
par(mfrow = c(1,2))
plot(residuals(HW.final), ylab = "Residuals", main = "Double Exponential Model")
acf(residuals(HW.final),lag.max = 100, main = "ACF Plot of Double Exponential Model")
```

## 4.3 Box-Jenkins Modelling

### 4.3.1 Model Proposal

```{r, fig.width=10, fig.height=2, fig.align='center', out.height='80%', echo=FALSE}
par(mfrow=c(1, 3), mar=c(4,4,3,3))
plot(JNJ.train, main = "Time Series Plot of JNJ Training Data")
acf(JNJ.train, lag.max = 36, main = "ACF Plot of JNJ Training Data")
pacf(JNJ.train, lag.max = 36, main = "PACF Plot of JNJ Training Data")
```

```{r, fig.width=10, fig.height=2, fig.align='center', out.height='80%', echo=FALSE}
diff.JNJ.train.seasonal = diff(JNJ.train, lag = 12)
par(mfrow=c(1, 3), mar=c(4,4,3,3))
plot(diff.JNJ.train.seasonal, main = "1 Time Seasonal Differencing (s = 12)")
acf(diff.JNJ.train.seasonal, lag.max = 36, main = "1 Time Seasonal Differencing (s = 12)")
pacf(diff.JNJ.train.seasonal, lag.max = 36, main = "1 Time Seasonal Differencing (s = 12)")
```


```{r, fig.width=12, fig.height=6, fig.align='center', out.width='78%', echo=FALSE}
diff.JNJ.train.seasonal.regular = diff(diff.JNJ.train.seasonal)
par(mfrow=c(2, 2), cex.main = 1.5)
plot(diff.JNJ.train.seasonal.regular, main = "Seasonal Differencing followed by Regular Differencing")
acf(diff.JNJ.train.seasonal.regular, lag.max = 36, main = "Seasonal Differencing followed by Regular Differencing")
pacf(diff.JNJ.train.seasonal.regular, lag.max = 36, main = "Seasonal Differencing followed by Regular Differencing")
acf(diff.JNJ.train.seasonal.regular^2, lag.max = 36, main = "(Seasonal Differencing followed by Regular Differencing)^2")
```

```{r, fig.width=12, fig.height=6, fig.align='center', out.width='78%', echo=FALSE}
diff.JNJ.train.regular = diff(JNJ.train)
par(mfrow=c(2, 2), cex.main = 1.5)
plot(diff.JNJ.train.regular, main = "1 Time Regular Differencing")
acf(diff.JNJ.train.regular, lag.max = 36, main = "1 Time Regular Differencing")
pacf(diff.JNJ.train.regular, lag.max = 36, main = "1 Time Regular Differencing")
acf(diff.JNJ.train.regular^2, lag.max = 36, main = "(1 Time Regular Differencing)^2")
```

$$
\begin{aligned}
&ARIMA(0,1,0), \ \ ARIMA(0,1,1), \ \ ARIMA(0,1,2), \\
&ARIMA(1,1,0), \ \ ARIMA(1,1,1), \ \ ARIMA(1,1,2), \\
&ARIMA(2,1,0), \ \ ARIMA(2,1,1), \ \ ARIMA(2,1,2).
\end{aligned}
$$

### 4.3.2 Model Diagnostics

```{r, include=FALSE, fig.width=12, fig.height=12}
# Seasonal Differencing
BJ.fit1 <- sarima(JNJ.train, p=2, d=0, q=0, P=0, D=1, Q=0, S=12)
# Seasonal Differencing followed by Regular Differencing
BJ.fit2 <- sarima(JNJ.train, p=0, d=1, q=1, P=0, D=1, Q=0, S=12)
BJ.fit3 <- sarima(JNJ.train, p=0, d=1, q=1, P=0, D=1, Q=1, S=12)
BJ.fit4 <- sarima(JNJ.train, p=0, d=1, q=1, P=1, D=1, Q=0, S=12)
BJ.fit5 <- sarima(JNJ.train, p=0, d=1, q=1, P=1, D=1, Q=1, S=12)
BJ.fit6 <- sarima(JNJ.train, p=0, d=1, q=2, P=0, D=1, Q=0, S=12)
BJ.fit7 <- sarima(JNJ.train, p=0, d=1, q=2, P=0, D=1, Q=1, S=12)
BJ.fit8 <- sarima(JNJ.train, p=0, d=1, q=2, P=1, D=1, Q=0, S=12)
BJ.fit9 <- sarima(JNJ.train, p=0, d=1, q=2, P=1, D=1, Q=1, S=12)
BJ.fit10 <- sarima(JNJ.train, p=1, d=1, q=1, P=0, D=1, Q=0, S=12)
BJ.fit11 <- sarima(JNJ.train, p=1, d=1, q=1, P=0, D=1, Q=1, S=12)
BJ.fit12 <- sarima(JNJ.train, p=1, d=1, q=1, P=1, D=1, Q=0, S=12)
BJ.fit13 <- sarima(JNJ.train, p=1, d=1, q=1, P=1, D=1, Q=1, S=12)
BJ.fit14 <- sarima(JNJ.train, p=1, d=1, q=2, P=0, D=1, Q=0, S=12)
BJ.fit15 <- sarima(JNJ.train, p=1, d=1, q=2, P=0, D=1, Q=1, S=12)
BJ.fit16 <- sarima(JNJ.train, p=1, d=1, q=2, P=1, D=1, Q=0, S=12)
BJ.fit17 <- sarima(JNJ.train, p=1, d=1, q=2, P=1, D=1, Q=1, S=12)
BJ.fit18 <- sarima(JNJ.train, p=2, d=1, q=1, P=0, D=1, Q=0, S=12)
BJ.fit19 <- sarima(JNJ.train, p=2, d=1, q=1, P=0, D=1, Q=1, S=12)
BJ.fit20 <- sarima(JNJ.train, p=2, d=1, q=1, P=1, D=1, Q=0, S=12)
BJ.fit21 <- sarima(JNJ.train, p=2, d=1, q=1, P=1, D=1, Q=1, S=12)
BJ.fit22 <- sarima(JNJ.train, p=2, d=1, q=2, P=0, D=1, Q=0, S=12)
BJ.fit23 <- sarima(JNJ.train, p=2, d=1, q=2, P=0, D=1, Q=1, S=12) #ok
BJ.fit24 <- sarima(JNJ.train, p=2, d=1, q=2, P=1, D=1, Q=0, S=12)
BJ.fit25 <- sarima(JNJ.train, p=2, d=1, q=2, P=1, D=1, Q=1, S=12)
# Regular Differencing
BJ.fit26 <- sarima(JNJ.train, p=0, d=1, q=0)
BJ.fit27 <- sarima(JNJ.train, p=0, d=1, q=1)
BJ.fit28 <- sarima(JNJ.train, p=0, d=1, q=2) #ok
BJ.fit29 <- sarima(JNJ.train, p=1, d=1, q=0)
BJ.fit30 <- sarima(JNJ.train, p=1, d=1, q=1)
BJ.fit31 <- sarima(JNJ.train, p=1, d=1, q=2)
BJ.fit32 <- sarima(JNJ.train, p=2, d=1, q=0) #ok
BJ.fit33 <- sarima(JNJ.train, p=2, d=1, q=1)
BJ.fit34 <- sarima(JNJ.train, p=2, d=1, q=2) #ok
```


```{r echo=FALSE, fig.height=4, fig.width=4, message=FALSE, warning=FALSE, out.width='50%', results='hide'}
par(mfrow = c(2,2), mar = c(1, 0, 1, 0))
BJ.fit23 = capture.output(sarima(JNJ.train, p=2, d=1, q=2, P=0, D=1, Q=1, S=12, cex = 0.2, cex.main = 0.8))
BJ.fit23 = sarima(JNJ.train, p=2, d=1, q=2, P=0, D=1, Q=1, S=12, details = F)
BJ.fit28 = capture.output(sarima(JNJ.train, p=0, d=1, q=2, cex = 0.2, cex.main = 0.8))
BJ.fit28 = sarima(JNJ.train, p=0, d=1, q=2, details = F)
BJ.fit32 = capture.output(sarima(JNJ.train, p=2, d=1, q=0, cex = 0.2, cex.main = 0.8))
BJ.fit32 = sarima(JNJ.train, p=2, d=1, q=0, details = F)
BJ.fit34 = capture.output(sarima(JNJ.train, p=2, d=1, q=2, cex = 0.2, cex.main = 0.8))
BJ.fit34 = sarima(JNJ.train, p=2, d=1, q=2, details = F)
```

### 4.3.3 Model Selection by Fit Quality and Prediction Power


```{r, echo=FALSE, fig.show='hide', warning=FALSE}
Models = c("SARIMA(2,1,2)×(0,1,1)[12]","ARIMA(0,1,2)","ARIMA(2,1,0)","ARIMA(2,1,2)")
AIC = c(BJ.fit23$ICs[1],BJ.fit28$ICs[1],BJ.fit32$ICs[1],BJ.fit34$ICs[1])
AICc = c(BJ.fit23$ICs[2],BJ.fit28$ICs[2],BJ.fit32$ICs[2],BJ.fit34$ICs[2])
BIC = c(BJ.fit23$ICs[3],BJ.fit28$ICs[3],BJ.fit32$ICs[3],BJ.fit34$ICs[3])
fore23 = sarima.for(JNJ.train, n.ahead=12, p=2, d=1, q=2, P=0, D=1, Q=1, S=12, pch = 16, cex = 0.9, col=c("seagreen"))
fore28 = sarima.for(JNJ.train, n.ahead=12, p=0, d=1, q=2, pch = 16, cex = 0.9, col=c("seagreen"))
fore32 = sarima.for(JNJ.train, n.ahead=12, p=2, d=1, q=0, pch = 16, cex = 0.9, col=c("seagreen"))
title(main = "ARIMA(2,1,0)", cex.main = 0.9)
fore34 = sarima.for(JNJ.train, n.ahead=12, p=2, d=1, q=2, pch = 16, cex = 0.9, col=c("seagreen"))
APSE = c(mean((JNJ.test - fore23$pred)^2),
         mean((JNJ.test - fore28$pred)^2),
         mean((JNJ.test - fore32$pred)^2),
         mean((JNJ.test - fore34$pred)^2))
BJ.FittedModels.ICs = data.frame(Models, AIC, AICc, BIC, APSE)
kable(BJ.FittedModels.ICs)
```


```{r, echo=FALSE, fig.width=10, fig.height=6, fig.align='center', warning=FALSE}
# prediction of y in validation set based on the model fitted to training set
## training set n.ahead 24 is just the validation set
par(mfrow=c(2,2))

fore23 = sarima.for(JNJ.train, n.ahead=12, p=2, d=1, q=2, P=0, D=1, Q=1, S=12, pch = 16, cex = 0.9, col=c("seagreen"))
title(main = "SARIMA(2,1,2)×(0,1,1)[12]", cex.main = 0.9)
lines(JNJ.test, col='dodgerblue', type='b', pch=18)
legend("topleft", lty = 1, bty = "n",
       c("Training Set", "Test Set", "Prediction",
         "90% Prediction Interval", "95% Prediction Interval"),
       col = c("seagreen", "dodgerblue", "red", "grey80", "grey50"))

fore28 = sarima.for(JNJ.train, n.ahead=12, p=0, d=1, q=2, pch = 16, cex = 0.9, col=c("seagreen"))
title(main = "ARIMA(0,1,2)", cex.main = 0.9)
lines(JNJ.test, col='dodgerblue', type='b', pch=18)
legend("topleft", lty = 1, bty = "n",
       c("Training Set", "Test Set", "Prediction",
         "90% Prediction Interval", "95% Prediction Interval"),
       col = c("seagreen", "dodgerblue", "red", "grey80", "grey50"))

fore32 = sarima.for(JNJ.train, n.ahead=12, p=2, d=1, q=0, pch = 16, cex = 0.9, col=c("seagreen"))
title(main = "ARIMA(2,1,0)", cex.main = 0.9)
lines(JNJ.test, col='dodgerblue', type='b', pch=18)
legend("topleft", lty = 1, bty = "n",
       c("Training Set", "Test Set", "Prediction",
         "90% Prediction Interval", "95% Prediction Interval"),
       col = c("seagreen", "dodgerblue", "red", "grey80", "grey50"))

fore34 = sarima.for(JNJ.train, n.ahead=12, p=2, d=1, q=2, pch = 16, cex = 0.9, col=c("seagreen"))
title(main = "ARIMA(2,1,2)", cex.main = 0.9)
lines(JNJ.test, col='dodgerblue', type='b', pch=18)
legend("topleft", lty = 1, bty = "n",
       c("Training Set", "Test Set", "Prediction",
         "90% Prediction Interval", "95% Prediction Interval"),
       col = c("seagreen", "dodgerblue", "red", "grey80", "grey50"))
```


# 5 Conclusion

## 5.1 Statistical Conclusions

```{r, fig.width=8, fig.height=4, fig.align='center', out.width='80%', echo=FALSE}
Final.Model = sarima.for(JNJ.ts, n.ahead=12, p=2, d=1, q=0, pch = 16, cex = 0.9, col=c("seagreen"))
title(main = "Forecasting JNJ Stock Price in One Year using ARIMA(2,1,0)", cex.main = 0.9)
lines(JNJ.test, col='dodgerblue', type='b', pch=18)
legend("topleft", lty = 1, bty = "n",
       c("Training Set", "Test Set", "Prediction",
         "90% Prediction Interval", "95% Prediction Interval"),
       col = c("seagreen", "dodgerblue", "red", "grey80", "grey50"))
```

